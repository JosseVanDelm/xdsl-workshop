{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56341c66",
   "metadata": {},
   "source": [
    "# Chapter 3: Toy IR\n",
    "\n",
    "Now that we're familiar with our language and the target instruction set, let's begin\n",
    "compiling step-by-step.\n",
    "\n",
    "## Introduction: Multi-Level Intermediate Representation\n",
    "\n",
    "xDSL leverages the MLIR representation of a program. MLIR specified a text format of\n",
    "this representation, which is useful for debugging and interoperation. For example,\n",
    "all the text in this format you'll see in this tutorial can be executed with the \n",
    "Toy language as compiled in the (MLIR Toy Tutorial)[https://mlir.llvm.org/docs/Tutorials/Toy/].\n",
    "\n",
    "Let's take a quick look at the MLIR IR representation of our example program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29e15305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"builtin.module\"() ({\n",
      "  \"toy.func\"() ({\n",
      "    %0 = \"toy.constant\"() {\"value\" = dense<[[1, 2, 3], [4, 5, 6]]> : tensor<2x3xi32>} : () -> tensor<2x3xi32>\n",
      "    %1 = \"toy.reshape\"(%0) : (tensor<2x3xi32>) -> tensor<2x3xi32>\n",
      "    %2 = \"toy.constant\"() {\"value\" = dense<[1, 2, 3, 4, 5, 6]> : tensor<6xi32>} : () -> tensor<6xi32>\n",
      "    %3 = \"toy.reshape\"(%2) : (tensor<6xi32>) -> tensor<3x2xi32>\n",
      "    %4 = \"toy.reshape\"(%3) : (tensor<3x2xi32>) -> tensor<2x3xi32>\n",
      "    %5 = \"toy.add\"(%1, %4) : (tensor<2x3xi32>, tensor<2x3xi32>) -> tensor<2x3xi32>\n",
      "    \"toy.print\"(%5) : (tensor<2x3xi32>) -> ()\n",
      "    \"toy.return\"() : () -> ()\n",
      "  }) {\"sym_name\" = \"main\", \"function_type\" = () -> ()} : () -> ()\n",
      "}) : () -> ()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from compiler import parse_toy, print_op\n",
    "\n",
    "example = \"\"\"\n",
    "def main() {\n",
    "  var a<2, 3> = [[1, 2, 3], [4, 5, 6]];\n",
    "  var b<3, 2> = [1, 2, 3, 4, 5, 6];\n",
    "  var c<2, 3> = b;\n",
    "  var d = a + c;\n",
    "  print(d);\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "toy_0 = parse_toy(example)\n",
    "print_op(toy_0)\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54bb582b",
   "metadata": {},
   "source": [
    "As you might have noticed, some parts look very similar to the Toy program above, and some\n",
    "things are added in. Let's look at the structure of an operation in the MLIR output before\n",
    "taking a close look at exactly what's going on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6251171e",
   "metadata": {},
   "source": [
    "## MLIR Syntax In Detail\n",
    "\n",
    "MLIR is designed to be a completely extensible infrastructure; there is no\n",
    "closed set of attributes (think: constant metadata), operations, or types. MLIR\n",
    "supports this extensibility with the concept of Dialects. Dialects provide a grouping \n",
    "mechanism for abstraction under a unique `namespace`.\n",
    "\n",
    "In MLIR, `Operations` are the core unit of abstraction and computation, similar in many \n",
    "ways to LLVM instructions. Operations can have application-specific semantics and can be \n",
    "used to represent all of the core IR structures in LLVM: instructions, globals (like \n",
    "functions), modules, etc.\n",
    "\n",
    "Here is the MLIR assembly for the Toy `transpose` operations:\n",
    "\n",
    "```mlir\n",
    "%t_tensor = \"toy.transpose\"(%tensor): (tensor<2x3xi32>) -> tensor<3x2xi32>\n",
    "```\n",
    "\n",
    "Let's break down the anatomy of this MLIR operation:\n",
    "\n",
    "-   `%t_tensor`\n",
    "\n",
    "    *   The name given to the result defined by this operation (which includes\n",
    "        [a prefixed sigil to avoid collisions](../../LangRef.md/#identifiers-and-keywords)).\n",
    "        An operation may define zero or more results (in the context of Toy, we\n",
    "        will limit ourselves to single-result operations), which are SSA values.\n",
    "        The name is used during parsing but is not persistent (e.g., it is not\n",
    "        tracked in the in-memory representation of the SSA value).\n",
    "\n",
    "-   `\"toy.transpose\"`\n",
    "\n",
    "    *   The name of the operation. It is expected to be a unique string, with\n",
    "        the namespace of the dialect prefixed before the \"`.`\". This can be read\n",
    "        as the `transpose` operation in the `toy` dialect.\n",
    "\n",
    "-   `(%tensor)`\n",
    "\n",
    "    *   A list of zero or more input operands (or arguments), which are SSA\n",
    "        values defined by other operations or referring to block arguments.\n",
    "\n",
    "\n",
    "-   `(tensor<2x3xf64>) -> tensor<3x2xf64>`\n",
    "\n",
    "    *   This refers to the type of the operation in a functional form, spelling\n",
    "        the types of the arguments in parentheses and the type of the return\n",
    "        values afterward.\n",
    "\n",
    "\n",
    "Shown here is the general form of an operation. As described above,\n",
    "the set of operations in MLIR is extensible. Operations are modeled\n",
    "using a small set of concepts, enabling operations to be reasoned\n",
    "about and manipulated generically. These concepts are:\n",
    "\n",
    "-   A name for the operation.\n",
    "-   A list of SSA operand values.\n",
    "-   A list of attributes.\n",
    "-   A list of types for result values.\n",
    "-   A source location for debugging purposes.\n",
    "-   A list of successors blocks (for branches, mostly).\n",
    "-   A list of regions (for structural operations like functions).\n",
    "\n",
    "## Defining Toy Operations\n",
    "\n",
    "Now that we have a `Toy` dialect, we can start defining the operations. This\n",
    "will allow for providing semantic information that the rest of the system can\n",
    "hook into. As an example, let's walk through the creation of a `toy.constant`\n",
    "operation. This operation will represent a constant value in the Toy language.\n",
    "\n",
    "```mlir\n",
    " %4 = \"toy.constant\"() {value = dense<[[1, 2, 3], [4, 5, 6]]> : tensor<2x3xi32>} : () -> tensor<2x3xi32>\n",
    "```\n",
    "\n",
    "This operation takes zero operands, a dense elements attribute named `value` \n",
    "to represent the constant value, and returns a single result of RankedTensorType. \n",
    "Let's take a look at the full definition and step through it in detail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96c2a0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%0 = \"toy.constant\"() {\"value\" = dense<[[1, 2, 3], [4, 5, 6]]> : tensor<2x3xi32>} : () -> tensor<2x3xi32>\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Annotated, TypeAlias\n",
    "\n",
    "# The xdsl.ir module implements the things we've mentioned in this chapter,\n",
    "# especially the equivalents of MLIR concepts.\n",
    "from xdsl.ir import Dialect, SSAValue, Attribute, Operation, OpResult\n",
    "\n",
    "# The builtin dialect is a collection of Operations, and Attributes that are expected\n",
    "# to be useful for most compilers, such as floating-point numbers, integers,\n",
    "# arrays, tensors, and more.\n",
    "from xdsl.dialects.builtin import TensorType, IntegerType, i32, DenseIntOrFPElementsAttr\n",
    "\n",
    "# xdsl.irdl provides a declarative Python API for Operation definitions\n",
    "from xdsl.irdl import OpAttr, irdl_op_definition\n",
    "\n",
    "\n",
    "\n",
    "TensorTypeI32: TypeAlias = TensorType[IntegerType]\n",
    "\n",
    "\n",
    "# A decorator to help implement some methods required by xDSL\n",
    "@irdl_op_definition\n",
    "class ConstantOp(Operation):\n",
    "    \"\"\"\n",
    "    Constant operation turns a literal into an SSA value. The data is attached\n",
    "    to the operation as an attribute. For example:\n",
    "\n",
    "    ```mlir\n",
    "      %0 = \"toy.constant\"() {\"value\" = dense<[[1, 2, 3], [4, 5, 6]]> : tensor<2x3xi32>} : () -> tensor<2x3xi32>\n",
    "    ```\n",
    "    \"\"\"\n",
    "    # Every operation has a name. The format is `dialect_name`.`operation_name`\n",
    "    name: str = \"toy.constant\"\n",
    "\n",
    "    # Attributes are defined using OpAttr, and can specify a type constraint on the attribute\n",
    "    value: OpAttr[DenseIntOrFPElementsAttr]\n",
    "\n",
    "    # The result type annotation uses `Annotated`, a type in the `typing` module that\n",
    "    # allows for runtime annotation of types with arbitrary values. xDSL leverages\n",
    "    # this annotation to populate a `verify()` method that will signal if there is\n",
    "    # a type mismatch during construction.\n",
    "    res: Annotated[OpResult, TensorTypeI32]\n",
    "\n",
    "    # Operations can provide helper constructors for ease of use\n",
    "    @staticmethod\n",
    "    def from_list(data: list[int], shape: list[int]) -> ConstantOp:\n",
    "        value = DenseIntOrFPElementsAttr.tensor_from_list(data, i32, shape)\n",
    "        return ConstantOp.create(attributes={'value': value}, result_types=[value.type])\n",
    "\n",
    "\n",
    "from compiler import print_op\n",
    "\n",
    "print_op(ConstantOp.from_list([1, 2, 3, 4, 5, 6], [2, 3]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2ce2423",
   "metadata": {},
   "source": [
    "## Another Look at the Generated Toy IR\n",
    "\n",
    "Let's take another look at the generated IR:\n",
    "\n",
    "``` MLIR\n",
    "\"builtin.module\"() ({\n",
    "  \"toy.func\"() ({\n",
    "    %0 = \"toy.constant\"() {\"value\" = dense<[[1, 2, 3], [4, 5, 6]]> : tensor<2x3xi32>} : () -> tensor<2x3xi32>\n",
    "    %1 = \"toy.reshape\"(%0) : (tensor<2x3xi32>) -> tensor<2x3xi32>\n",
    "    %2 = \"toy.constant\"() {\"value\" = dense<[1, 2, 3, 4, 5, 6]> : tensor<6xi32>} : () -> tensor<6xi32>\n",
    "    %3 = \"toy.reshape\"(%2) : (tensor<6xi32>) -> tensor<3x2xi32>\n",
    "    %4 = \"toy.reshape\"(%3) : (tensor<3x2xi32>) -> tensor<2x3xi32>\n",
    "    %5 = \"toy.add\"(%1, %4) : (tensor<2x3xi32>, tensor<2x3xi32>) -> tensor<2x3xi32>\n",
    "    \"toy.print\"(%5) : (tensor<2x3xi32>) -> ()\n",
    "    \"toy.return\"() : () -> ()\n",
    "  }) {\"sym_name\" = \"main\", \"function_type\" = () -> ()} : () -> ()\n",
    "}) : () -> ()\n",
    "```\n",
    "\n",
    "First thing we see is that the whole program is wrapped in `builtin.module`. Modules\n",
    "roughly represent a parsed input file. Nested one level deep is a `toy.func`, representing\n",
    "the `main` function defined in the source. Inside it are a list of instructions.\n",
    "\n",
    "The next four lines of MLIR ops correspond to the first two lines of the Toy program.\n",
    "\n",
    "``` MLIR\n",
    "%0 = \"toy.constant\"() {\"value\" = dense<[[1, 2, 3], [4, 5, 6]]> : tensor<2x3xi32>} : () -> tensor<2x3xi32>\n",
    "%1 = \"toy.reshape\"(%0) : (tensor<2x3xi32>) -> tensor<2x3xi32>\n",
    "%2 = \"toy.constant\"() {\"value\" = dense<[1, 2, 3, 4, 5, 6]> : tensor<6xi32>} : () -> tensor<6xi32>\n",
    "%3 = \"toy.reshape\"(%2) : (tensor<6xi32>) -> tensor<3x2xi32>\n",
    "```\n",
    "\n",
    "``` Python\n",
    "var a<2, 3> = [[1, 2, 3], [4, 5, 6]];\n",
    "var b<3, 2> = [1, 2, 3, 4, 5, 6];\n",
    "```\n",
    "\n",
    "Because the types on the left of the `=` operator might not correspond to the type of the \n",
    "literal, the reshape operations are inserted. Most of the time the shapes will match,\n",
    "and the reshape will be redundant. Let's take a look at how to optimise our code to remove\n",
    "redundant transposes using the xDSL infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776b23d5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "8c189b674810d96af66833b07bd9a3fc1b15f37c11bf66e9bb86bed0f17aed6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
