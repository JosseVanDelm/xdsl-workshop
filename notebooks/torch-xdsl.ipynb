{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing a torch model in MLIR Syntax\n",
    "\n",
    "Those can already be generated by [Torch-MLIR](https://github.com/llvm/torch-mlir)!\n",
    "\n",
    "Let's just parse it and print it for now\n",
    "\n",
    "One can see that some tensor literals are only used in transpose operations. Let's optimize this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"builtin.module\"() ({\n",
      "  \"func.func\"() ({\n",
      "  ^0(%0 : #torch.vtensor<[1 : i64, 3 : i64, 224 : i64, 224 : i64], f32>):\n",
      "    %1 = \"torch.constant.int\"() {\"value\" = 0 : i64} : () -> #torch.int\n",
      "    %2 = \"torch.constant.int\"() {\"value\" = 1 : i64} : () -> #torch.int\n",
      "    %3 = \"torch.constant.float\"() {\"value\" = 1.0 : f64} : () -> #torch.float\n",
      "    %4 = \"torch.constant.int\"() {\"value\" = -1 : i64} : () -> #torch.int\n",
      "    %5 = \"torch.constant.bool\"() {\"value\" = true} : () -> #torch.bool\n",
      "    %6 = \"torch.constant.bool\"() {\"value\" = false} : () -> #torch.bool\n",
      "    %7 = \"torch.constant.none\"() : () -> #torch.none\n",
      "    %8 = \"torch.vtensor.literal\"() {\"value\" = dense_resource<__elided__> : tensor<1000x4096xf32>} : () -> #torch.vtensor<[1000 : i64, 4096 : i64], f32>\n",
      "    %9 = \"torch.vtensor.literal\"() {\"value\" = dense_resource<__elided__> : tensor<1000xf32>} : () -> #torch.vtensor<[1000 : i64], f32>\n",
      "    %10 = \"torch.vtensor.literal\"() {\"value\" = dense_resource<__elided__> : tensor<4096x4096xf32>} : () -> #torch.vtensor<[4096 : i64, 4096 : i64], f32>\n",
      "    %11 = \"torch.vtensor.literal\"() {\"value\" = dense_resource<__elided__> : tensor<4096xf32>} : () -> #torch.vtensor<[4096 : i64], f32>\n",
      "    %12 = \"torch.vtensor.literal\"() {\"value\" = dense_resource<__elided__> : tensor<4096x9216xf32>} : () -> #torch.vtensor<[4096 : i64, 9216 : i64], f32>\n",
      "    %13 = \"torch.vtensor.literal\"() {\"value\" = dense_resource<__elided__> : tensor<4096xf32>} : () -> #torch.vtensor<[4096 : i64], f32>\n",
      "    %14 = \"torch.vtensor.literal\"() {\"value\" = dense_resource<__elided__> : tensor<256x256x3x3xf32>} : () -> #torch.vtensor<[256 : i64, 256 : i64, 3 : i64, 3 : i64], f32>\n",
      "    %15 = \"torch.vtensor.literal\"() {\"value\" = dense_resource<__elided__> : tensor<256xf32>} : () -> #torch.vtensor<[256 : i64], f32>\n",
      "    %16 = \"torch.vtensor.literal\"() {\"value\" = dense_resource<__elided__> : tensor<256x384x3x3xf32>} : () -> #torch.vtensor<[256 : i64, 384 : i64, 3 : i64, 3 : i64], f32>\n",
      "    %17 = \"torch.vtensor.literal\"() {\"value\" = dense_resource<__elided__> : tensor<256xf32>} : () -> #torch.vtensor<[256 : i64], f32>\n",
      "    %18 = \"torch.vtensor.literal\"() {\"value\" = dense_resource<__elided__> : tensor<384x192x3x3xf32>} : () -> #torch.vtensor<[384 : i64, 192 : i64, 3 : i64, 3 : i64], f32>\n",
      "    %19 = \"torch.vtensor.literal\"() {\"value\" = dense_resource<__elided__> : tensor<384xf32>} : () -> #torch.vtensor<[384 : i64], f32>\n",
      "    %20 = \"torch.vtensor.literal\"() {\"value\" = dense_resource<__elided__> : tensor<192x64x5x5xf32>} : () -> #torch.vtensor<[192 : i64, 64 : i64, 5 : i64, 5 : i64], f32>\n",
      "    %21 = \"torch.vtensor.literal\"() {\"value\" = dense_resource<__elided__> : tensor<192xf32>} : () -> #torch.vtensor<[192 : i64], f32>\n",
      "    %22 = \"torch.vtensor.literal\"() {\"value\" = dense_resource<__elided__> : tensor<64x3x11x11xf32>} : () -> #torch.vtensor<[64 : i64, 3 : i64, 11 : i64, 11 : i64], f32>\n",
      "    %23 = \"torch.vtensor.literal\"() {\"value\" = dense_resource<__elided__> : tensor<64xf32>} : () -> #torch.vtensor<[64 : i64], f32>\n",
      "    %24 = \"torch.constant.int\"() {\"value\" = 4 : i64} : () -> #torch.int\n",
      "    %25 = \"torch.constant.int\"() {\"value\" = 2 : i64} : () -> #torch.int\n",
      "    %26 = \"torch.constant.int\"() {\"value\" = 3 : i64} : () -> #torch.int\n",
      "    %27 = \"torch.prim.ListConstruct\"(%24, %24) : (#torch.int, #torch.int) -> #torch.list<#torch.int>\n",
      "    %28 = \"torch.prim.ListConstruct\"(%25, %25) : (#torch.int, #torch.int) -> #torch.list<#torch.int>\n",
      "    %29 = \"torch.prim.ListConstruct\"(%2, %2) : (#torch.int, #torch.int) -> #torch.list<#torch.int>\n",
      "    %30 = \"torch.prim.ListConstruct\"(%1, %1) : (#torch.int, #torch.int) -> #torch.list<#torch.int>\n",
      "    %31 = \"torch.aten.convolution\"(%0, %22, %23, %27, %28, %29, %6, %30, %2) : (#torch.vtensor<[1 : i64, 3 : i64, 224 : i64, 224 : i64], f32>, #torch.vtensor<[64 : i64, 3 : i64, 11 : i64, 11 : i64], f32>, #torch.vtensor<[64 : i64], f32>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.bool, #torch.list<#torch.int>, #torch.int) -> #torch.vtensor<[1 : i64, 64 : i64, 55 : i64, 55 : i64], f32>\n",
      "    %32 = \"torch.aten.relu\"(%31) : (#torch.vtensor<[1 : i64, 64 : i64, 55 : i64, 55 : i64], f32>) -> #torch.vtensor<[1 : i64, 64 : i64, 55 : i64, 55 : i64], f32>\n",
      "    %33 = \"torch.prim.ListConstruct\"(%26, %26) : (#torch.int, #torch.int) -> #torch.list<#torch.int>\n",
      "    %34 = \"torch.aten.max_pool2d\"(%32, %33, %28, %30, %29, %6) : (#torch.vtensor<[1 : i64, 64 : i64, 55 : i64, 55 : i64], f32>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.bool) -> #torch.vtensor<[1 : i64, 64 : i64, 27 : i64, 27 : i64], f32>\n",
      "    %35 = \"torch.aten.convolution\"(%34, %20, %21, %29, %28, %29, %6, %30, %2) : (#torch.vtensor<[1 : i64, 64 : i64, 27 : i64, 27 : i64], f32>, #torch.vtensor<[192 : i64, 64 : i64, 5 : i64, 5 : i64], f32>, #torch.vtensor<[192 : i64], f32>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.bool, #torch.list<#torch.int>, #torch.int) -> #torch.vtensor<[1 : i64, 192 : i64, 27 : i64, 27 : i64], f32>\n",
      "    %36 = \"torch.aten.relu\"(%35) : (#torch.vtensor<[1 : i64, 192 : i64, 27 : i64, 27 : i64], f32>) -> #torch.vtensor<[1 : i64, 192 : i64, 27 : i64, 27 : i64], f32>\n",
      "    %37 = \"torch.aten.max_pool2d\"(%36, %33, %28, %30, %29, %6) : (#torch.vtensor<[1 : i64, 192 : i64, 27 : i64, 27 : i64], f32>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.bool) -> #torch.vtensor<[1 : i64, 192 : i64, 13 : i64, 13 : i64], f32>\n",
      "    %38 = \"torch.aten.convolution\"(%37, %18, %19, %29, %29, %29, %6, %30, %2) : (#torch.vtensor<[1 : i64, 192 : i64, 13 : i64, 13 : i64], f32>, #torch.vtensor<[384 : i64, 192 : i64, 3 : i64, 3 : i64], f32>, #torch.vtensor<[384 : i64], f32>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.bool, #torch.list<#torch.int>, #torch.int) -> #torch.vtensor<[1 : i64, 384 : i64, 13 : i64, 13 : i64], f32>\n",
      "    %39 = \"torch.aten.relu\"(%38) : (#torch.vtensor<[1 : i64, 384 : i64, 13 : i64, 13 : i64], f32>) -> #torch.vtensor<[1 : i64, 384 : i64, 13 : i64, 13 : i64], f32>\n",
      "    %40 = \"torch.aten.convolution\"(%39, %16, %17, %29, %29, %29, %6, %30, %2) : (#torch.vtensor<[1 : i64, 384 : i64, 13 : i64, 13 : i64], f32>, #torch.vtensor<[256 : i64, 384 : i64, 3 : i64, 3 : i64], f32>, #torch.vtensor<[256 : i64], f32>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.bool, #torch.list<#torch.int>, #torch.int) -> #torch.vtensor<[1 : i64, 256 : i64, 13 : i64, 13 : i64], f32>\n",
      "    %41 = \"torch.aten.relu\"(%40) : (#torch.vtensor<[1 : i64, 256 : i64, 13 : i64, 13 : i64], f32>) -> #torch.vtensor<[1 : i64, 256 : i64, 13 : i64, 13 : i64], f32>\n",
      "    %42 = \"torch.aten.convolution\"(%41, %14, %15, %29, %29, %29, %6, %30, %2) : (#torch.vtensor<[1 : i64, 256 : i64, 13 : i64, 13 : i64], f32>, #torch.vtensor<[256 : i64, 256 : i64, 3 : i64, 3 : i64], f32>, #torch.vtensor<[256 : i64], f32>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.bool, #torch.list<#torch.int>, #torch.int) -> #torch.vtensor<[1 : i64, 256 : i64, 13 : i64, 13 : i64], f32>\n",
      "    %43 = \"torch.aten.relu\"(%42) : (#torch.vtensor<[1 : i64, 256 : i64, 13 : i64, 13 : i64], f32>) -> #torch.vtensor<[1 : i64, 256 : i64, 13 : i64, 13 : i64], f32>\n",
      "    %44 = \"torch.aten.max_pool2d\"(%43, %33, %28, %30, %29, %6) : (#torch.vtensor<[1 : i64, 256 : i64, 13 : i64, 13 : i64], f32>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.bool) -> #torch.vtensor<[1 : i64, 256 : i64, 6 : i64, 6 : i64], f32>\n",
      "    \"torch.runtime.assert\"(%5) {\"message\" = \"unimplemented: only support cases where input and output size are equal for non-unit output size\"} : (#torch.bool) -> ()\n",
      "    \"torch.runtime.assert\"(%5) {\"message\" = \"unimplemented: only support cases where input and output size are equal for non-unit output size\"} : (#torch.bool) -> ()\n",
      "    %45 = \"torch.prim.ListConstruct\"(%2, %2) : (#torch.int, #torch.int) -> #torch.list<#torch.int>\n",
      "    %46 = \"torch.prim.ListConstruct\"(%2, %2) : (#torch.int, #torch.int) -> #torch.list<#torch.int>\n",
      "    %47 = \"torch.prim.ListConstruct\"(%1, %1) : (#torch.int, #torch.int) -> #torch.list<#torch.int>\n",
      "    %48 = \"torch.aten.avg_pool2d\"(%44, %45, %46, %47, %6, %5, %7) : (#torch.vtensor<[1 : i64, 256 : i64, 6 : i64, 6 : i64], f32>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.bool, #torch.bool, #torch.none) -> #torch.vtensor<[1 : i64, 256 : i64, 6 : i64, 6 : i64], f32>\n",
      "    %49 = \"torch.prim.ListConstruct\"(%2, %4) : (#torch.int, #torch.int) -> #torch.list<#torch.int>\n",
      "    %50 = \"torch.aten.view\"(%48, %49) : (#torch.vtensor<[1 : i64, 256 : i64, 6 : i64, 6 : i64], f32>, #torch.list<#torch.int>) -> #torch.vtensor<[1 : i64, 9216 : i64], f32>\n",
      "    %51 = \"torch.aten.transpose.int\"(%12, %1, %2) : (#torch.vtensor<[4096 : i64, 9216 : i64], f32>, #torch.int, #torch.int) -> #torch.vtensor<[9216 : i64, 4096 : i64], f32>\n",
      "    %52 = \"torch.aten.mm\"(%50, %51) : (#torch.vtensor<[1 : i64, 9216 : i64], f32>, #torch.vtensor<[9216 : i64, 4096 : i64], f32>) -> #torch.vtensor<[1 : i64, 4096 : i64], f32>\n",
      "    %53 = \"torch.aten.add.Tensor\"(%52, %13, %3) : (#torch.vtensor<[1 : i64, 4096 : i64], f32>, #torch.vtensor<[4096 : i64], f32>, #torch.float) -> #torch.vtensor<[1 : i64, 4096 : i64], f32>\n",
      "    %54 = \"torch.aten.relu\"(%53) : (#torch.vtensor<[1 : i64, 4096 : i64], f32>) -> #torch.vtensor<[1 : i64, 4096 : i64], f32>\n",
      "    %55 = \"torch.aten.transpose.int\"(%10, %1, %2) : (#torch.vtensor<[4096 : i64, 4096 : i64], f32>, #torch.int, #torch.int) -> #torch.vtensor<[4096 : i64, 4096 : i64], f32>\n",
      "    %56 = \"torch.aten.mm\"(%54, %55) : (#torch.vtensor<[1 : i64, 4096 : i64], f32>, #torch.vtensor<[4096 : i64, 4096 : i64], f32>) -> #torch.vtensor<[1 : i64, 4096 : i64], f32>\n",
      "    %57 = \"torch.aten.add.Tensor\"(%56, %11, %3) : (#torch.vtensor<[1 : i64, 4096 : i64], f32>, #torch.vtensor<[4096 : i64], f32>, #torch.float) -> #torch.vtensor<[1 : i64, 4096 : i64], f32>\n",
      "    %58 = \"torch.aten.relu\"(%57) : (#torch.vtensor<[1 : i64, 4096 : i64], f32>) -> #torch.vtensor<[1 : i64, 4096 : i64], f32>\n",
      "    %59 = \"torch.aten.transpose.int\"(%8, %1, %2) : (#torch.vtensor<[1000 : i64, 4096 : i64], f32>, #torch.int, #torch.int) -> #torch.vtensor<[4096 : i64, 1000 : i64], f32>\n",
      "    %60 = \"torch.aten.mm\"(%58, %59) : (#torch.vtensor<[1 : i64, 4096 : i64], f32>, #torch.vtensor<[4096 : i64, 1000 : i64], f32>) -> #torch.vtensor<[1 : i64, 1000 : i64], f32>\n",
      "    %61 = \"torch.aten.add.Tensor\"(%60, %9, %3) : (#torch.vtensor<[1 : i64, 1000 : i64], f32>, #torch.vtensor<[1000 : i64], f32>, #torch.float) -> #torch.vtensor<[1 : i64, 1000 : i64], f32>\n",
      "    \"func.return\"(%61) : (#torch.vtensor<[1 : i64, 1000 : i64], f32>) -> ()\n",
      "  }) {\"function_type\" = (#torch.vtensor<[1 : i64, 3 : i64, 224 : i64, 224 : i64], f32>) -> #torch.vtensor<[1 : i64, 1000 : i64], f32>, \"sym_name\" = \"forward\"} : () -> ()\n",
      "}) {\"torch.debug_module_name\" = \"AlexNet\"} : () -> ()\n"
     ]
    }
   ],
   "source": [
    "import xdsl, riscemu\n",
    "from torchxdsl.dialect import *\n",
    "\n",
    "from xdsl.dialects.func import Func\n",
    "from xdsl.dialects.builtin import Builtin\n",
    "from xdsl.parser import Parser, Source\n",
    "\n",
    "from compiler import print_op\n",
    "from xdsl.ir import MLContext\n",
    "\n",
    "context = MLContext()\n",
    "context.register_dialect(Torch)\n",
    "context.register_dialect(Func)\n",
    "context.register_dialect(Builtin)\n",
    "\n",
    "with open('examples/alexnet.mlir')as f:\n",
    "    parser = Parser(context, f.read(), Source.MLIR, f.name)\n",
    "    module = parser.parse_module()\n",
    "\n",
    "print_op(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"builtin.module\"() ({\n",
      "  \"func.func\"() ({\n",
      "  ^0(%0 : #torch.vtensor<[1 : i64, 3 : i64, 224 : i64, 224 : i64], f32>):\n",
      "    %1 = \"torch.constant.int\"() {\"value\" = 0 : i64} : () -> #torch.int\n",
      "    %2 = \"torch.constant.int\"() {\"value\" = 1 : i64} : () -> #torch.int\n",
      "    %3 = \"torch.constant.float\"() {\"value\" = 1.0 : f64} : () -> #torch.float\n",
      "    %4 = \"torch.constant.int\"() {\"value\" = -1 : i64} : () -> #torch.int\n",
      "    %5 = \"torch.constant.bool\"() {\"value\" = true} : () -> #torch.bool\n",
      "    %6 = \"torch.constant.bool\"() {\"value\" = false} : () -> #torch.bool\n",
      "    %7 = \"torch.constant.none\"() : () -> #torch.none\n",
      "    %8 = \"torch.vtensor.literal\"() {\"value\" = dense_resource<__elided__> : tensor<1000xf32>} : () -> #torch.vtensor<[1000 : i64], f32>\n",
      "    %9 = \"torch.vtensor.literal\"() {\"value\" = dense_resource<__elided__> : tensor<4096xf32>} : () -> #torch.vtensor<[4096 : i64], f32>\n",
      "    %10 = \"torch.vtensor.literal\"() {\"value\" = dense_resource<__elided__> : tensor<4096xf32>} : () -> #torch.vtensor<[4096 : i64], f32>\n",
      "    %11 = \"torch.vtensor.literal\"() {\"value\" = dense_resource<__elided__> : tensor<256x256x3x3xf32>} : () -> #torch.vtensor<[256 : i64, 256 : i64, 3 : i64, 3 : i64], f32>\n",
      "    %12 = \"torch.vtensor.literal\"() {\"value\" = dense_resource<__elided__> : tensor<256xf32>} : () -> #torch.vtensor<[256 : i64], f32>\n",
      "    %13 = \"torch.vtensor.literal\"() {\"value\" = dense_resource<__elided__> : tensor<256x384x3x3xf32>} : () -> #torch.vtensor<[256 : i64, 384 : i64, 3 : i64, 3 : i64], f32>\n",
      "    %14 = \"torch.vtensor.literal\"() {\"value\" = dense_resource<__elided__> : tensor<256xf32>} : () -> #torch.vtensor<[256 : i64], f32>\n",
      "    %15 = \"torch.vtensor.literal\"() {\"value\" = dense_resource<__elided__> : tensor<384x192x3x3xf32>} : () -> #torch.vtensor<[384 : i64, 192 : i64, 3 : i64, 3 : i64], f32>\n",
      "    %16 = \"torch.vtensor.literal\"() {\"value\" = dense_resource<__elided__> : tensor<384xf32>} : () -> #torch.vtensor<[384 : i64], f32>\n",
      "    %17 = \"torch.vtensor.literal\"() {\"value\" = dense_resource<__elided__> : tensor<192x64x5x5xf32>} : () -> #torch.vtensor<[192 : i64, 64 : i64, 5 : i64, 5 : i64], f32>\n",
      "    %18 = \"torch.vtensor.literal\"() {\"value\" = dense_resource<__elided__> : tensor<192xf32>} : () -> #torch.vtensor<[192 : i64], f32>\n",
      "    %19 = \"torch.vtensor.literal\"() {\"value\" = dense_resource<__elided__> : tensor<64x3x11x11xf32>} : () -> #torch.vtensor<[64 : i64, 3 : i64, 11 : i64, 11 : i64], f32>\n",
      "    %20 = \"torch.vtensor.literal\"() {\"value\" = dense_resource<__elided__> : tensor<64xf32>} : () -> #torch.vtensor<[64 : i64], f32>\n",
      "    %21 = \"torch.constant.int\"() {\"value\" = 4 : i64} : () -> #torch.int\n",
      "    %22 = \"torch.constant.int\"() {\"value\" = 2 : i64} : () -> #torch.int\n",
      "    %23 = \"torch.constant.int\"() {\"value\" = 3 : i64} : () -> #torch.int\n",
      "    %24 = \"torch.prim.ListConstruct\"(%21, %21) : (#torch.int, #torch.int) -> #torch.list<#torch.int>\n",
      "    %25 = \"torch.prim.ListConstruct\"(%22, %22) : (#torch.int, #torch.int) -> #torch.list<#torch.int>\n",
      "    %26 = \"torch.prim.ListConstruct\"(%2, %2) : (#torch.int, #torch.int) -> #torch.list<#torch.int>\n",
      "    %27 = \"torch.prim.ListConstruct\"(%1, %1) : (#torch.int, #torch.int) -> #torch.list<#torch.int>\n",
      "    %28 = \"torch.aten.convolution\"(%0, %19, %20, %24, %25, %26, %6, %27, %2) : (#torch.vtensor<[1 : i64, 3 : i64, 224 : i64, 224 : i64], f32>, #torch.vtensor<[64 : i64, 3 : i64, 11 : i64, 11 : i64], f32>, #torch.vtensor<[64 : i64], f32>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.bool, #torch.list<#torch.int>, #torch.int) -> #torch.vtensor<[1 : i64, 64 : i64, 55 : i64, 55 : i64], f32>\n",
      "    %29 = \"torch.aten.relu\"(%28) : (#torch.vtensor<[1 : i64, 64 : i64, 55 : i64, 55 : i64], f32>) -> #torch.vtensor<[1 : i64, 64 : i64, 55 : i64, 55 : i64], f32>\n",
      "    %30 = \"torch.prim.ListConstruct\"(%23, %23) : (#torch.int, #torch.int) -> #torch.list<#torch.int>\n",
      "    %31 = \"torch.aten.max_pool2d\"(%29, %30, %25, %27, %26, %6) : (#torch.vtensor<[1 : i64, 64 : i64, 55 : i64, 55 : i64], f32>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.bool) -> #torch.vtensor<[1 : i64, 64 : i64, 27 : i64, 27 : i64], f32>\n",
      "    %32 = \"torch.aten.convolution\"(%31, %17, %18, %26, %25, %26, %6, %27, %2) : (#torch.vtensor<[1 : i64, 64 : i64, 27 : i64, 27 : i64], f32>, #torch.vtensor<[192 : i64, 64 : i64, 5 : i64, 5 : i64], f32>, #torch.vtensor<[192 : i64], f32>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.bool, #torch.list<#torch.int>, #torch.int) -> #torch.vtensor<[1 : i64, 192 : i64, 27 : i64, 27 : i64], f32>\n",
      "    %33 = \"torch.aten.relu\"(%32) : (#torch.vtensor<[1 : i64, 192 : i64, 27 : i64, 27 : i64], f32>) -> #torch.vtensor<[1 : i64, 192 : i64, 27 : i64, 27 : i64], f32>\n",
      "    %34 = \"torch.aten.max_pool2d\"(%33, %30, %25, %27, %26, %6) : (#torch.vtensor<[1 : i64, 192 : i64, 27 : i64, 27 : i64], f32>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.bool) -> #torch.vtensor<[1 : i64, 192 : i64, 13 : i64, 13 : i64], f32>\n",
      "    %35 = \"torch.aten.convolution\"(%34, %15, %16, %26, %26, %26, %6, %27, %2) : (#torch.vtensor<[1 : i64, 192 : i64, 13 : i64, 13 : i64], f32>, #torch.vtensor<[384 : i64, 192 : i64, 3 : i64, 3 : i64], f32>, #torch.vtensor<[384 : i64], f32>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.bool, #torch.list<#torch.int>, #torch.int) -> #torch.vtensor<[1 : i64, 384 : i64, 13 : i64, 13 : i64], f32>\n",
      "    %36 = \"torch.aten.relu\"(%35) : (#torch.vtensor<[1 : i64, 384 : i64, 13 : i64, 13 : i64], f32>) -> #torch.vtensor<[1 : i64, 384 : i64, 13 : i64, 13 : i64], f32>\n",
      "    %37 = \"torch.aten.convolution\"(%36, %13, %14, %26, %26, %26, %6, %27, %2) : (#torch.vtensor<[1 : i64, 384 : i64, 13 : i64, 13 : i64], f32>, #torch.vtensor<[256 : i64, 384 : i64, 3 : i64, 3 : i64], f32>, #torch.vtensor<[256 : i64], f32>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.bool, #torch.list<#torch.int>, #torch.int) -> #torch.vtensor<[1 : i64, 256 : i64, 13 : i64, 13 : i64], f32>\n",
      "    %38 = \"torch.aten.relu\"(%37) : (#torch.vtensor<[1 : i64, 256 : i64, 13 : i64, 13 : i64], f32>) -> #torch.vtensor<[1 : i64, 256 : i64, 13 : i64, 13 : i64], f32>\n",
      "    %39 = \"torch.aten.convolution\"(%38, %11, %12, %26, %26, %26, %6, %27, %2) : (#torch.vtensor<[1 : i64, 256 : i64, 13 : i64, 13 : i64], f32>, #torch.vtensor<[256 : i64, 256 : i64, 3 : i64, 3 : i64], f32>, #torch.vtensor<[256 : i64], f32>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.bool, #torch.list<#torch.int>, #torch.int) -> #torch.vtensor<[1 : i64, 256 : i64, 13 : i64, 13 : i64], f32>\n",
      "    %40 = \"torch.aten.relu\"(%39) : (#torch.vtensor<[1 : i64, 256 : i64, 13 : i64, 13 : i64], f32>) -> #torch.vtensor<[1 : i64, 256 : i64, 13 : i64, 13 : i64], f32>\n",
      "    %41 = \"torch.aten.max_pool2d\"(%40, %30, %25, %27, %26, %6) : (#torch.vtensor<[1 : i64, 256 : i64, 13 : i64, 13 : i64], f32>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.bool) -> #torch.vtensor<[1 : i64, 256 : i64, 6 : i64, 6 : i64], f32>\n",
      "    \"torch.runtime.assert\"(%5) {\"message\" = \"unimplemented: only support cases where input and output size are equal for non-unit output size\"} : (#torch.bool) -> ()\n",
      "    \"torch.runtime.assert\"(%5) {\"message\" = \"unimplemented: only support cases where input and output size are equal for non-unit output size\"} : (#torch.bool) -> ()\n",
      "    %42 = \"torch.prim.ListConstruct\"(%2, %2) : (#torch.int, #torch.int) -> #torch.list<#torch.int>\n",
      "    %43 = \"torch.prim.ListConstruct\"(%2, %2) : (#torch.int, #torch.int) -> #torch.list<#torch.int>\n",
      "    %44 = \"torch.prim.ListConstruct\"(%1, %1) : (#torch.int, #torch.int) -> #torch.list<#torch.int>\n",
      "    %45 = \"torch.aten.avg_pool2d\"(%41, %42, %43, %44, %6, %5, %7) : (#torch.vtensor<[1 : i64, 256 : i64, 6 : i64, 6 : i64], f32>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.bool, #torch.bool, #torch.none) -> #torch.vtensor<[1 : i64, 256 : i64, 6 : i64, 6 : i64], f32>\n",
      "    %46 = \"torch.prim.ListConstruct\"(%2, %4) : (#torch.int, #torch.int) -> #torch.list<#torch.int>\n",
      "    %47 = \"torch.aten.view\"(%45, %46) : (#torch.vtensor<[1 : i64, 256 : i64, 6 : i64, 6 : i64], f32>, #torch.list<#torch.int>) -> #torch.vtensor<[1 : i64, 9216 : i64], f32>\n",
      "    %48 = \"torch.vtensor.literal\"() {\"value\" = dense_resource<__elided__> : tensor<4096x9216xf32>} : () -> #torch.vtensor<[9216 : i64, 4096 : i64], f32>\n",
      "    %49 = \"torch.aten.mm\"(%47, %48) : (#torch.vtensor<[1 : i64, 9216 : i64], f32>, #torch.vtensor<[9216 : i64, 4096 : i64], f32>) -> #torch.vtensor<[1 : i64, 4096 : i64], f32>\n",
      "    %50 = \"torch.aten.add.Tensor\"(%49, %10, %3) : (#torch.vtensor<[1 : i64, 4096 : i64], f32>, #torch.vtensor<[4096 : i64], f32>, #torch.float) -> #torch.vtensor<[1 : i64, 4096 : i64], f32>\n",
      "    %51 = \"torch.aten.relu\"(%50) : (#torch.vtensor<[1 : i64, 4096 : i64], f32>) -> #torch.vtensor<[1 : i64, 4096 : i64], f32>\n",
      "    %52 = \"torch.vtensor.literal\"() {\"value\" = dense_resource<__elided__> : tensor<4096x4096xf32>} : () -> #torch.vtensor<[4096 : i64, 4096 : i64], f32>\n",
      "    %53 = \"torch.aten.mm\"(%51, %52) : (#torch.vtensor<[1 : i64, 4096 : i64], f32>, #torch.vtensor<[4096 : i64, 4096 : i64], f32>) -> #torch.vtensor<[1 : i64, 4096 : i64], f32>\n",
      "    %54 = \"torch.aten.add.Tensor\"(%53, %9, %3) : (#torch.vtensor<[1 : i64, 4096 : i64], f32>, #torch.vtensor<[4096 : i64], f32>, #torch.float) -> #torch.vtensor<[1 : i64, 4096 : i64], f32>\n",
      "    %55 = \"torch.aten.relu\"(%54) : (#torch.vtensor<[1 : i64, 4096 : i64], f32>) -> #torch.vtensor<[1 : i64, 4096 : i64], f32>\n",
      "    %56 = \"torch.vtensor.literal\"() {\"value\" = dense_resource<__elided__> : tensor<1000x4096xf32>} : () -> #torch.vtensor<[4096 : i64, 1000 : i64], f32>\n",
      "    %57 = \"torch.aten.mm\"(%55, %56) : (#torch.vtensor<[1 : i64, 4096 : i64], f32>, #torch.vtensor<[4096 : i64, 1000 : i64], f32>) -> #torch.vtensor<[1 : i64, 1000 : i64], f32>\n",
      "    %58 = \"torch.aten.add.Tensor\"(%57, %8, %3) : (#torch.vtensor<[1 : i64, 1000 : i64], f32>, #torch.vtensor<[1000 : i64], f32>, #torch.float) -> #torch.vtensor<[1 : i64, 1000 : i64], f32>\n",
      "    \"func.return\"(%58) : (#torch.vtensor<[1 : i64, 1000 : i64], f32>) -> ()\n",
      "  }) {\"function_type\" = (#torch.vtensor<[1 : i64, 3 : i64, 224 : i64, 224 : i64], f32>) -> #torch.vtensor<[1 : i64, 1000 : i64], f32>, \"sym_name\" = \"forward\"} : () -> ()\n",
      "}) {\"torch.debug_module_name\" = \"AlexNet\"} : () -> ()\n"
     ]
    }
   ],
   "source": [
    "# Import some things from the xdsl.pattern_rewriter module:\n",
    "from xdsl.pattern_rewriter import (GreedyRewritePatternApplier,\n",
    "                                   PatternRewriter, PatternRewriteWalker,\n",
    "                                   RewritePattern, op_type_rewrite_pattern)\n",
    "\n",
    "# Create our rewriter class:\n",
    "class TransposedLiteralOptimizer(RewritePattern):\n",
    "    \n",
    "    @op_type_rewrite_pattern\n",
    "    def match_and_rewrite(self, transpose: TransposeOp, rewriter: PatternRewriter):\n",
    "        \"\"\"\n",
    "        This method will be called on each TransposeOp in our Torch-xDSL module.\n",
    "        \"\"\"\n",
    "        # we iterate over all operands (arguments) of the add instruction\n",
    "        if isinstance(transpose.tensor.op, VTensorLitteralOp):\n",
    "            \n",
    "            transposed_literal = transpose.tensor.op.clone()\n",
    "            t = transposed_literal.res.typ.dimensions.data[transpose.dim1.op.value.value.data]\n",
    "            transposed_literal.res.typ.dimensions.data[transpose.dim1.op.value.value.data] = transposed_literal.res.typ.dimensions.data[transpose.dim2.op.value.value.data]\n",
    "            transposed_literal.res.typ.dimensions.data[transpose.dim2.op.value.value.data] = t\n",
    "\n",
    "            rewriter.replace_matched_op(transposed_literal)\n",
    "            if len(transpose.tensor.uses) == 0:\n",
    "                rewriter.erase_op(transpose.tensor.op)\n",
    "            \n",
    "optimized_module = module.clone()\n",
    "PatternRewriteWalker(TransposedLiteralOptimizer()).rewrite_module(optimized_module)\n",
    "print_op(optimized_module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
