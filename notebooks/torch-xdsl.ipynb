{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing a torch model in MLIR Syntax\n",
    "\n",
    "Those can already be generated by [Torch-MLIR](https://github.com/llvm/torch-mlir)!\n",
    "\n",
    "Let's just parse it and print it for now\n",
    "\n",
    "One can see that some tensor literals are only used in transpose operations. Let's optimize this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Unexpected error in backtracking:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/papychacal/.local/lib/python3.10/site-packages/xdsl/parser.py\", line 321, in backtracking\n",
      "    try:\n",
      "  File \"/home/papychacal/.local/lib/python3.10/site-packages/xdsl/parser.py\", line 1278, in try_parse_builtin_named_attr\n",
      "    'dense': self._parse_builtin_dense_attr,\n",
      "  File \"/home/papychacal/.local/lib/python3.10/site-packages/xdsl/parser.py\", line 1276, in not_implemented\n",
      "    self.tokenizer.consume_peeked(name)\n",
      "NotImplementedError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/papychacal/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3442, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_6163/3355953327.py\", line 17, in <module>\n",
      "    module = parser.parse_module()\n",
      "  File \"/home/papychacal/.local/lib/python3.10/site-packages/xdsl/parser.py\", line 622, in parse_module\n",
      "  File \"/home/papychacal/.local/lib/python3.10/site-packages/xdsl/parser.py\", line 1067, in raise_error\n",
      "    at_position = self.tokenizer.next_token(peek=True)\n",
      "xdsl.utils.exceptions.ParseError: examples/alexnet.mlir:11:44\n",
      "    %7 = \"torch.vtensor.literal\"() {value = dense_resource<__elided__> : tensor<1000x4096xf32>} : () -> !torch.vtensor<[1000,4096],f32>\n",
      "                                            ^^^^^^^^^^^^^^\n",
      "                                            Unexpected exception: \n",
      "\n",
      "examples/alexnet.mlir:11:44\n",
      "    %7 = \"torch.vtensor.literal\"() {value = dense_resource<__elided__> : tensor<1000x4096xf32>} : () -> !torch.vtensor<[1000,4096],f32>\n",
      "                                            ^^^^^^^^^^^^^^\n",
      "                                            Unknown attribute (neither builtin nor dialect could be parsed)!\n",
      "\n",
      "examples/alexnet.mlir:11:4\n",
      "    %7 = \"torch.vtensor.literal\"() {value = dense_resource<__elided__> : tensor<1000x4096xf32>} : () -> !torch.vtensor<[1000,4096],f32>\n",
      "    ^\n",
      "    Reached end of region, expected `}`!\n",
      "\n",
      "examples/alexnet.mlir:2:2\n",
      "  \"func.func\"() ({\n",
      "  ^\n",
      "  Reached end of region, expected `}`!\n",
      "\n",
      "examples/alexnet.mlir:1:0\n",
      "\"builtin.module\"() ({\n",
      "^\n",
      "Could not parse entire input!\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/papychacal/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2057, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/papychacal/.local/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/papychacal/.local/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/papychacal/.local/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/papychacal/.local/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"/home/papychacal/.local/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"/home/papychacal/.local/lib/python3.10/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/papychacal/.local/lib/python3.10/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/home/papychacal/.local/lib/python3.10/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/papychacal/.local/lib/python3.10/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/home/papychacal/.local/lib/python3.10/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/papychacal/.local/lib/python3.10/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/home/papychacal/.local/lib/python3.10/site-packages/executing/executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "from torchxdsl.dialect import *\n",
    "\n",
    "from xdsl.dialects.func import Func\n",
    "from xdsl.dialects.builtin import Builtin\n",
    "from xdsl.parser import Parser, Source\n",
    "\n",
    "from compiler import print_op\n",
    "from xdsl.ir import MLContext\n",
    "\n",
    "context = MLContext()\n",
    "context.register_dialect(Torch)\n",
    "context.register_dialect(Func)\n",
    "context.register_dialect(Builtin)\n",
    "\n",
    "with open('examples/alexnet.mlir')as f:\n",
    "    parser = Parser(context, f.read(), Source.MLIR, f.name)\n",
    "    module = parser.parse_module()\n",
    "\n",
    "print_op(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"builtin.module\"() ({\n",
      "  \"func.func\"() ({\n",
      "  ^0(%0 : #torch.vtensor<[1 : i64, 3 : i64, 224 : i64, 224 : i64], f32>):\n",
      "    %1 = \"torch.constant.int\"() {\"value\" = 0 : i64} : () -> #torch.int\n",
      "    %2 = \"torch.constant.int\"() {\"value\" = 1 : i64} : () -> #torch.int\n",
      "    %3 = \"torch.constant.float\"() {\"value\" = 1.0 : f64} : () -> #torch.float\n",
      "    %4 = \"torch.constant.int\"() {\"value\" = -1 : i64} : () -> #torch.int\n",
      "    %5 = \"torch.constant.bool\"() {\"value\" = true} : () -> #torch.bool\n",
      "    %6 = \"torch.constant.bool\"() {\"value\" = false} : () -> #torch.bool\n",
      "    %7 = \"torch.constant.none\"() : () -> #torch.none\n",
      "    %8 = \"torch.vtensor.literal\"() {\"value\" = dense<[1.0]> : tensor<1000xf32>} : () -> #torch.vtensor<[1000 : i64], f32>\n",
      "    %9 = \"torch.vtensor.literal\"() {\"value\" = dense<[1.0]> : tensor<4096xf32>} : () -> #torch.vtensor<[4096 : i64], f32>\n",
      "    %10 = \"torch.vtensor.literal\"() {\"value\" = dense<[1.0]> : tensor<4096xf32>} : () -> #torch.vtensor<[4096 : i64], f32>\n",
      "    %11 = \"torch.vtensor.literal\"() {\"value\" = dense<[1.0]> : tensor<256x256x3x3xf32>} : () -> #torch.vtensor<[256 : i64, 256 : i64, 3 : i64, 3 : i64], f32>\n",
      "    %12 = \"torch.vtensor.literal\"() {\"value\" = dense<[1.0]> : tensor<256xf32>} : () -> #torch.vtensor<[256 : i64], f32>\n",
      "    %13 = \"torch.vtensor.literal\"() {\"value\" = dense<[1.0]> : tensor<256x384x3x3xf32>} : () -> #torch.vtensor<[256 : i64, 384 : i64, 3 : i64, 3 : i64], f32>\n",
      "    %14 = \"torch.vtensor.literal\"() {\"value\" = dense<[1.0]> : tensor<256xf32>} : () -> #torch.vtensor<[256 : i64], f32>\n",
      "    %15 = \"torch.vtensor.literal\"() {\"value\" = dense<[1.0]> : tensor<384x192x3x3xf32>} : () -> #torch.vtensor<[384 : i64, 192 : i64, 3 : i64, 3 : i64], f32>\n",
      "    %16 = \"torch.vtensor.literal\"() {\"value\" = dense<[1.0]> : tensor<384xf32>} : () -> #torch.vtensor<[384 : i64], f32>\n",
      "    %17 = \"torch.vtensor.literal\"() {\"value\" = dense<[1.0]> : tensor<192x64x5x5xf32>} : () -> #torch.vtensor<[192 : i64, 64 : i64, 5 : i64, 5 : i64], f32>\n",
      "    %18 = \"torch.vtensor.literal\"() {\"value\" = dense<[1.0]> : tensor<192xf32>} : () -> #torch.vtensor<[192 : i64], f32>\n",
      "    %19 = \"torch.vtensor.literal\"() {\"value\" = dense<[1.0]> : tensor<64x3x11x11xf32>} : () -> #torch.vtensor<[64 : i64, 3 : i64, 11 : i64, 11 : i64], f32>\n",
      "    %20 = \"torch.vtensor.literal\"() {\"value\" = dense<[1.0]> : tensor<64xf32>} : () -> #torch.vtensor<[64 : i64], f32>\n",
      "    %21 = \"torch.constant.int\"() {\"value\" = 4 : i64} : () -> #torch.int\n",
      "    %22 = \"torch.constant.int\"() {\"value\" = 2 : i64} : () -> #torch.int\n",
      "    %23 = \"torch.constant.int\"() {\"value\" = 3 : i64} : () -> #torch.int\n",
      "    %24 = \"torch.prim.ListConstruct\"(%21, %21) : (#torch.int, #torch.int) -> #torch.list<#torch.int>\n",
      "    %25 = \"torch.prim.ListConstruct\"(%22, %22) : (#torch.int, #torch.int) -> #torch.list<#torch.int>\n",
      "    %26 = \"torch.prim.ListConstruct\"(%2, %2) : (#torch.int, #torch.int) -> #torch.list<#torch.int>\n",
      "    %27 = \"torch.prim.ListConstruct\"(%1, %1) : (#torch.int, #torch.int) -> #torch.list<#torch.int>\n",
      "    %28 = \"torch.aten.convolution\"(%0, %19, %20, %24, %25, %26, %6, %27, %2) : (#torch.vtensor<[1 : i64, 3 : i64, 224 : i64, 224 : i64], f32>, #torch.vtensor<[64 : i64, 3 : i64, 11 : i64, 11 : i64], f32>, #torch.vtensor<[64 : i64], f32>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.bool, #torch.list<#torch.int>, #torch.int) -> #torch.vtensor<[1 : i64, 64 : i64, 55 : i64, 55 : i64], f32>\n",
      "    %29 = \"torch.aten.relu\"(%28) : (#torch.vtensor<[1 : i64, 64 : i64, 55 : i64, 55 : i64], f32>) -> #torch.vtensor<[1 : i64, 64 : i64, 55 : i64, 55 : i64], f32>\n",
      "    %30 = \"torch.prim.ListConstruct\"(%23, %23) : (#torch.int, #torch.int) -> #torch.list<#torch.int>\n",
      "    %31 = \"torch.aten.max_pool2d\"(%29, %30, %25, %27, %26, %6) : (#torch.vtensor<[1 : i64, 64 : i64, 55 : i64, 55 : i64], f32>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.bool) -> #torch.vtensor<[1 : i64, 64 : i64, 27 : i64, 27 : i64], f32>\n",
      "    %32 = \"torch.aten.convolution\"(%31, %17, %18, %26, %25, %26, %6, %27, %2) : (#torch.vtensor<[1 : i64, 64 : i64, 27 : i64, 27 : i64], f32>, #torch.vtensor<[192 : i64, 64 : i64, 5 : i64, 5 : i64], f32>, #torch.vtensor<[192 : i64], f32>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.bool, #torch.list<#torch.int>, #torch.int) -> #torch.vtensor<[1 : i64, 192 : i64, 27 : i64, 27 : i64], f32>\n",
      "    %33 = \"torch.aten.relu\"(%32) : (#torch.vtensor<[1 : i64, 192 : i64, 27 : i64, 27 : i64], f32>) -> #torch.vtensor<[1 : i64, 192 : i64, 27 : i64, 27 : i64], f32>\n",
      "    %34 = \"torch.aten.max_pool2d\"(%33, %30, %25, %27, %26, %6) : (#torch.vtensor<[1 : i64, 192 : i64, 27 : i64, 27 : i64], f32>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.bool) -> #torch.vtensor<[1 : i64, 192 : i64, 13 : i64, 13 : i64], f32>\n",
      "    %35 = \"torch.aten.convolution\"(%34, %15, %16, %26, %26, %26, %6, %27, %2) : (#torch.vtensor<[1 : i64, 192 : i64, 13 : i64, 13 : i64], f32>, #torch.vtensor<[384 : i64, 192 : i64, 3 : i64, 3 : i64], f32>, #torch.vtensor<[384 : i64], f32>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.bool, #torch.list<#torch.int>, #torch.int) -> #torch.vtensor<[1 : i64, 384 : i64, 13 : i64, 13 : i64], f32>\n",
      "    %36 = \"torch.aten.relu\"(%35) : (#torch.vtensor<[1 : i64, 384 : i64, 13 : i64, 13 : i64], f32>) -> #torch.vtensor<[1 : i64, 384 : i64, 13 : i64, 13 : i64], f32>\n",
      "    %37 = \"torch.aten.convolution\"(%36, %13, %14, %26, %26, %26, %6, %27, %2) : (#torch.vtensor<[1 : i64, 384 : i64, 13 : i64, 13 : i64], f32>, #torch.vtensor<[256 : i64, 384 : i64, 3 : i64, 3 : i64], f32>, #torch.vtensor<[256 : i64], f32>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.bool, #torch.list<#torch.int>, #torch.int) -> #torch.vtensor<[1 : i64, 256 : i64, 13 : i64, 13 : i64], f32>\n",
      "    %38 = \"torch.aten.relu\"(%37) : (#torch.vtensor<[1 : i64, 256 : i64, 13 : i64, 13 : i64], f32>) -> #torch.vtensor<[1 : i64, 256 : i64, 13 : i64, 13 : i64], f32>\n",
      "    %39 = \"torch.aten.convolution\"(%38, %11, %12, %26, %26, %26, %6, %27, %2) : (#torch.vtensor<[1 : i64, 256 : i64, 13 : i64, 13 : i64], f32>, #torch.vtensor<[256 : i64, 256 : i64, 3 : i64, 3 : i64], f32>, #torch.vtensor<[256 : i64], f32>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.bool, #torch.list<#torch.int>, #torch.int) -> #torch.vtensor<[1 : i64, 256 : i64, 13 : i64, 13 : i64], f32>\n",
      "    %40 = \"torch.aten.relu\"(%39) : (#torch.vtensor<[1 : i64, 256 : i64, 13 : i64, 13 : i64], f32>) -> #torch.vtensor<[1 : i64, 256 : i64, 13 : i64, 13 : i64], f32>\n",
      "    %41 = \"torch.aten.max_pool2d\"(%40, %30, %25, %27, %26, %6) : (#torch.vtensor<[1 : i64, 256 : i64, 13 : i64, 13 : i64], f32>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.bool) -> #torch.vtensor<[1 : i64, 256 : i64, 6 : i64, 6 : i64], f32>\n",
      "    \"torch.runtime.assert\"(%5) {\"message\" = \"unimplemented: only support cases where input and output size are equal for non-unit output size\"} : (#torch.bool) -> ()\n",
      "    \"torch.runtime.assert\"(%5) {\"message\" = \"unimplemented: only support cases where input and output size are equal for non-unit output size\"} : (#torch.bool) -> ()\n",
      "    %42 = \"torch.prim.ListConstruct\"(%2, %2) : (#torch.int, #torch.int) -> #torch.list<#torch.int>\n",
      "    %43 = \"torch.prim.ListConstruct\"(%2, %2) : (#torch.int, #torch.int) -> #torch.list<#torch.int>\n",
      "    %44 = \"torch.prim.ListConstruct\"(%1, %1) : (#torch.int, #torch.int) -> #torch.list<#torch.int>\n",
      "    %45 = \"torch.aten.avg_pool2d\"(%41, %42, %43, %44, %6, %5, %7) : (#torch.vtensor<[1 : i64, 256 : i64, 6 : i64, 6 : i64], f32>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.list<#torch.int>, #torch.bool, #torch.bool, #torch.none) -> #torch.vtensor<[1 : i64, 256 : i64, 6 : i64, 6 : i64], f32>\n",
      "    %46 = \"torch.prim.ListConstruct\"(%2, %4) : (#torch.int, #torch.int) -> #torch.list<#torch.int>\n",
      "    %47 = \"torch.aten.view\"(%45, %46) : (#torch.vtensor<[1 : i64, 256 : i64, 6 : i64, 6 : i64], f32>, #torch.list<#torch.int>) -> #torch.vtensor<[1 : i64, 9216 : i64], f32>\n",
      "    %48 = \"torch.vtensor.literal\"() {\"value\" = dense<[1.0]> : tensor<4096x9216xf32>} : () -> #torch.vtensor<[4096 : i64, 9216 : i64], f32>\n",
      "    %49 = \"torch.aten.mm\"(%47, %48) : (#torch.vtensor<[1 : i64, 9216 : i64], f32>, #torch.vtensor<[4096 : i64, 9216 : i64], f32>) -> #torch.vtensor<[1 : i64, 4096 : i64], f32>\n",
      "    %50 = \"torch.aten.add.Tensor\"(%49, %10, %3) : (#torch.vtensor<[1 : i64, 4096 : i64], f32>, #torch.vtensor<[4096 : i64], f32>, #torch.float) -> #torch.vtensor<[1 : i64, 4096 : i64], f32>\n",
      "    %51 = \"torch.aten.relu\"(%50) : (#torch.vtensor<[1 : i64, 4096 : i64], f32>) -> #torch.vtensor<[1 : i64, 4096 : i64], f32>\n",
      "    %52 = \"torch.vtensor.literal\"() {\"value\" = dense<[1.0]> : tensor<4096x4096xf32>} : () -> #torch.vtensor<[4096 : i64, 4096 : i64], f32>\n",
      "    %53 = \"torch.aten.mm\"(%51, %52) : (#torch.vtensor<[1 : i64, 4096 : i64], f32>, #torch.vtensor<[4096 : i64, 4096 : i64], f32>) -> #torch.vtensor<[1 : i64, 4096 : i64], f32>\n",
      "    %54 = \"torch.aten.add.Tensor\"(%53, %9, %3) : (#torch.vtensor<[1 : i64, 4096 : i64], f32>, #torch.vtensor<[4096 : i64], f32>, #torch.float) -> #torch.vtensor<[1 : i64, 4096 : i64], f32>\n",
      "    %55 = \"torch.aten.relu\"(%54) : (#torch.vtensor<[1 : i64, 4096 : i64], f32>) -> #torch.vtensor<[1 : i64, 4096 : i64], f32>\n",
      "    %56 = \"torch.vtensor.literal\"() {\"value\" = dense<[1.0]> : tensor<1000x4096xf32>} : () -> #torch.vtensor<[1000 : i64, 4096 : i64], f32>\n",
      "    %57 = \"torch.aten.mm\"(%55, %56) : (#torch.vtensor<[1 : i64, 4096 : i64], f32>, #torch.vtensor<[1000 : i64, 4096 : i64], f32>) -> #torch.vtensor<[1 : i64, 1000 : i64], f32>\n",
      "    %58 = \"torch.aten.add.Tensor\"(%57, %8, %3) : (#torch.vtensor<[1 : i64, 1000 : i64], f32>, #torch.vtensor<[1000 : i64], f32>, #torch.float) -> #torch.vtensor<[1 : i64, 1000 : i64], f32>\n",
      "    \"func.return\"(%58) : (#torch.vtensor<[1 : i64, 1000 : i64], f32>) -> ()\n",
      "  }) {\"function_type\" = (#torch.vtensor<[1 : i64, 3 : i64, 224 : i64, 224 : i64], f32>) -> #torch.vtensor<[1 : i64, 1000 : i64], f32>, \"sym_name\" = \"forward\"} : () -> ()\n",
      "}) {\"torch.debug_module_name\" = \"AlexNet\"} : () -> ()\n"
     ]
    }
   ],
   "source": [
    "# Import some things from the xdsl.pattern_rewriter module:\n",
    "from xdsl.pattern_rewriter import (GreedyRewritePatternApplier,\n",
    "                                   PatternRewriter, PatternRewriteWalker,\n",
    "                                   RewritePattern, op_type_rewrite_pattern)\n",
    "\n",
    "# Create our rewriter class:\n",
    "class TransposedLiteralOptimizer(RewritePattern):\n",
    "    \n",
    "    @op_type_rewrite_pattern\n",
    "    def match_and_rewrite(self, transpose: TransposeOp, rewriter: PatternRewriter):\n",
    "        \"\"\"\n",
    "        This method will be called on each TransposeOp in our Torch-xDSL module.\n",
    "        \"\"\"\n",
    "        # we iterate over all operands (arguments) of the add instruction\n",
    "        if isinstance(transpose.tensor.op, VTensorLitteralOp):\n",
    "            \n",
    "            transposed_literal = transpose.tensor.op.clone()\n",
    "            t = transposed_literal.res.typ.dimensions.data[transpose.dim1.op.value.value.data]\n",
    "            transposed_literal.res.typ.dimensions.data[transpose.dim1.op.value.value.data] = transposed_literal.res.typ.dimensions.data[transpose.dim2.op.value.value.data]\n",
    "            transposed_literal.res.typ.dimensions.data[transpose.dim2.op.value.value.data] = t\n",
    "\n",
    "            rewriter.replace_matched_op(transposed_literal)\n",
    "            if len(transpose.tensor.uses) == 0:\n",
    "                rewriter.erase_op(transpose.tensor.op)\n",
    "            \n",
    "optimized_module = module.clone()\n",
    "PatternRewriteWalker(TransposedLiteralOptimizer()).rewrite_module(optimized_module)\n",
    "print_op(optimized_module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
